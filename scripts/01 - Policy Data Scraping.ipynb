{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Unstructured Policy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://alternative-fuels-observatory.ec.europa.eu\"\n",
    "\n",
    "def scrape_country_policy(url, country_name):\n",
    "    \"\"\"Scrape incentives/legislation text for one country.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    main_section = soup.find(\"main\")\n",
    "    policy_text = []\n",
    "    \n",
    "    # Grab headings, paragraphs, bullet points\n",
    "    for elem in main_section.find_all([\"h2\", \"h3\", \"p\", \"li\"]):\n",
    "        text = elem.get_text(strip=True)\n",
    "        if text:\n",
    "            policy_text.append(text)\n",
    "    \n",
    "    return {\n",
    "        \"country\": country_name,\n",
    "        \"policy_text\": \" \".join(policy_text)\n",
    "    }\n",
    "\n",
    "# --- Step 1: Scrape mother page for country links\n",
    "mother_url = \"https://alternative-fuels-observatory.ec.europa.eu/transport-mode/road\"\n",
    "res = requests.get(mother_url)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# find all <a> tags with \"/road/{country}/incentives-legislations\"\n",
    "links = soup.find_all(\"a\", href=True)\n",
    "country_links = []\n",
    "for a in links:\n",
    "    href = a[\"href\"]\n",
    "    if href.startswith(\"/transport-mode/road/\") and href.endswith(\"incentives-legislations\"):\n",
    "        country_name = a.get_text(strip=True)\n",
    "        full_url = BASE_URL + href\n",
    "        country_links.append((country_name, full_url))\n",
    "\n",
    "print(f\"Found {len(country_links)} country pages\")\n",
    "\n",
    "# --- Step 2: Loop over countries\n",
    "all_data = []\n",
    "for name, url in country_links:\n",
    "    try:\n",
    "        print(f\"Scraping {name}...\")\n",
    "        data = scrape_country_policy(url, name)\n",
    "        all_data.append(data)\n",
    "        time.sleep(1)  # polite delay\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed for {name}: {e}\")\n",
    "\n",
    "# --- Step 3: Store results\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"raw_data/policy_incentives.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved to raw_data/policy_incentives.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
